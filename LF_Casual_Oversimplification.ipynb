{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LF_Casual_Oversimplification.ipynb",
      "provenance": [],
      "mount_file_id": "1hTUADSWn6sRNTX14kTtvqKf4YEn_W0f2",
      "authorship_tag": "ABX9TyN529XhSaREPj6DBv7RlDLu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajay1994/Logical_Fallacy_Detection/blob/master/LF_Casual_Oversimplification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDlgLT_rU6HH",
        "colab_type": "code",
        "outputId": "c4662bee-e40a-4ffb-a04a-5dee17bde3d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.chdir('drive/My Drive/Cognitive Vigilance/')\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Cognitive Vigilance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJu-IQaBcXKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "6790d624-9723-4879-8459-d537e460f688"
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (5.2.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (47.1.1)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXLHCtx0ZHjE",
        "colab_type": "code",
        "outputId": "d149707b-6697-43ae-bb19-e3409a461365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from datasets.myutils import *\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpbd1iKRZVH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractions_dict = {\"'cause\": 'because',\n",
        " \"I'd\": 'I had / I would',\n",
        " \"I'd've\": 'I would have',\n",
        " \"I'll\": 'I shall / I will',\n",
        " \"I'll've\": 'I shall have / I will have',\n",
        " \"I'm\": 'I am',\n",
        " \"I've\": 'I have',\n",
        " \"ain't\": 'am not / are not / is not / has not / have not',\n",
        " \"aren't\": 'are not / am not',\n",
        " \"can't\": 'cannot',\n",
        " \"can't've\": 'cannot have',\n",
        " \"could've\": 'could have',\n",
        " \"couldn't\": 'could not',\n",
        " \"couldn't've\": 'could not have',\n",
        " \"didn't\": 'did not',\n",
        " \"doesn't\": 'does not',\n",
        " \"don't\": 'do not',\n",
        " \"hadn't\": 'had not',\n",
        " \"hadn't've\": 'had not have',\n",
        " \"hasn't\": 'has not',\n",
        " \"haven't\": 'have not',\n",
        " \"he'd\": 'he had / he would',\n",
        " \"he'd've\": 'he would have',\n",
        " \"he'll\": 'he shall / he will',\n",
        " \"he'll've\": 'he shall have / he will have',\n",
        " \"he's\": 'he has / he is',\n",
        " \"how'd\": 'how did',\n",
        " \"how'd'y\": 'how do you',\n",
        " \"how'll\": 'how will',\n",
        " \"how's\": 'how has / how is / how does',\n",
        " \"isn't\": 'is not',\n",
        " \"it'd\": 'it had / it would',\n",
        " \"it'd've\": 'it would have',\n",
        " \"it'll\": 'it shall / it will',\n",
        " \"it'll've\": 'it shall have / it will have',\n",
        " \"it's\": 'it has / it is',\n",
        " \"let's\": 'let us',\n",
        " \"ma'am\": 'madam',\n",
        " \"mayn't\": 'may not',\n",
        " \"might've\": 'might have',\n",
        " \"mightn't\": 'might not',\n",
        " \"mightn't've\": 'might not have',\n",
        " \"must've\": 'must have',\n",
        " \"mustn't\": 'must not',\n",
        " \"mustn't've\": 'must not have',\n",
        " \"needn't\": 'need not',\n",
        " \"needn't've\": 'need not have',\n",
        " \"o'clock\": 'of the clock',\n",
        " \"oughtn't\": 'ought not',\n",
        " \"oughtn't've\": 'ought not have',\n",
        " \"sha'n't\": 'shall not',\n",
        " \"shan't\": 'shall not',\n",
        " \"shan't've\": 'shall not have',\n",
        " \"she'd\": 'she had / she would',\n",
        " \"she'd've\": 'she would have',\n",
        " \"she'll\": 'she shall / she will',\n",
        " \"she'll've\": 'she shall have / she will have',\n",
        " \"she's\": 'she has / she is',\n",
        " \"should've\": 'should have',\n",
        " \"shouldn't\": 'should not',\n",
        " \"shouldn't've\": 'should not have',\n",
        " \"so's\": 'so as / so is',\n",
        " \"so've\": 'so have',\n",
        " \"that'd\": 'that would / that had',\n",
        " \"that'd've\": 'that would have',\n",
        " \"that's\": 'that has / that is',\n",
        " \"there'd\": 'there had / there would',\n",
        " \"there'd've\": 'there would have',\n",
        " \"there's\": 'there has / there is',\n",
        " \"they'd\": 'they had / they would',\n",
        " \"they'd've\": 'they would have',\n",
        " \"they'll\": 'they shall / they will',\n",
        " \"they'll've\": 'they shall have / they will have',\n",
        " \"they're\": 'they are',\n",
        " \"they've\": 'they have',\n",
        " \"to've\": 'to have',\n",
        " \"wasn't\": 'was not',\n",
        " \"we'd\": 'we had / we would',\n",
        " \"we'd've\": 'we would have',\n",
        " \"we'll\": 'we will',\n",
        " \"we'll've\": 'we will have',\n",
        " \"we're\": 'we are',\n",
        " \"we've\": 'we have',\n",
        " \"weren't\": 'were not',\n",
        " \"what'll\": 'what shall / what will',\n",
        " \"what'll've\": 'what shall have / what will have',\n",
        " \"what're\": 'what are',\n",
        " \"what's\": 'what has / what is',\n",
        " \"what've\": 'what have',\n",
        " \"when's\": 'when has / when is',\n",
        " \"when've\": 'when have',\n",
        " \"where'd\": 'where did',\n",
        " \"where's\": 'where has / where is',\n",
        " \"where've\": 'where have',\n",
        " \"who'll\": 'who shall / who will',\n",
        " \"who'll've\": 'who shall have / who will have',\n",
        " \"who's\": 'who has / who is',\n",
        " \"who've\": 'who have',\n",
        " \"why's\": 'why has / why is',\n",
        " \"why've\": 'why have',\n",
        " \"will've\": 'will have',\n",
        " \"won't\": 'will not',\n",
        " \"won't've\": 'will not have',\n",
        " \"would've\": 'would have',\n",
        " \"wouldn't\": 'would not',\n",
        " \"wouldn't've\": 'would not have',\n",
        " \"y'all\": 'you all',\n",
        " \"y'all'd\": 'you all would',\n",
        " \"y'all'd've\": 'you all would have',\n",
        " \"y'all're\": 'you all are',\n",
        " \"y'all've\": 'you all have',\n",
        " \"you'd\": 'you had / you would',\n",
        " \"you'd've\": 'you would have',\n",
        " \"you'll\": 'you shall / you will',\n",
        " \"you'll've\": 'you shall have / you will have',\n",
        " \"you're\": 'you are',\n",
        " \"you've\": 'you have'}\n",
        "\n",
        "\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H0tr4BQuCs8",
        "colab_type": "code",
        "outputId": "07926f83-e8d1-4e73-c91b-7d1a0a386f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "contractions_re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "re.compile(r\"('cause|I'd|I'd've|I'll|I'll've|I'm|I've|ain't|aren't|can't|can't've|could've|couldn't|couldn't've|didn't|doesn't|don't|hadn't|hadn't've|hasn't|haven't|he'd|he'd've|he'll|he'll've|he's|how'd|how'd'y|how'll|how's|isn't|it'd|it'd've|it'll|it'll've|it's|let's|ma'am|mayn't|might've|mightn't|mightn't've|must've|mustn't|mustn't've|needn't|needn't've|o'clock|oughtn't|oughtn't've|sha'n't|shan't|shan't've|she'd|she'd've|she'll|she'll've|she's|should've|shouldn't|shouldn't've|so's|so've|that'd|that'd've|that's|there'd|there'd've|there's|they'd|they'd've|they'll|they'll've|they're|they've|to've|wasn't|we'd|we'd've|we'll|we'll've|we're|we've|weren't|what'll|what'll've|what're|what's|what've|when's|when've|where'd|where's|where've|who'll|who'll've|who's|who've|why's|why've|will've|won't|won't've|would've|wouldn't|wouldn't've|y'all|y'all'd|y'all'd've|y'all're|y'all've|you'd|you'd've|you'll|you'll've|you're|you've)\",\n",
              "re.UNICODE)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMvA7L_LZU-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expand_contractions(s, contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIuKyyeNZU7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaner(s):\n",
        "    s= re.sub(r'[^\\x00-\\x7F]','',s) #remove ascii\n",
        "    s=s.replace('•',' ')\n",
        "    s=s.replace('\\n\\n','.\\n') #replace double \\n with full stops and \\n\n",
        "    s = re.sub('\\.\\.+', '.', s) #remove multiple dots\n",
        "    s=s.replace('\\n',' ') #remove newliner\n",
        "    s=re.sub(r'[a-z]\\.', r'\\g<0> ', s)\n",
        "    s=re.sub(r'(?<!\\w)([A-Z])\\.', r'\\1', s) #remove periods from abbrevations\n",
        "    s=s.replace(u'\\xa0', u' ')\n",
        "    s=re.sub(' +',' ',s) #remove extra whitespaces\n",
        "    #s=s.lower()\n",
        "    s=expand_contractions(s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_kv6zQdZU3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str_punc_to_be_removed = \"!#$%&()*+-/:;<=>?@[\\]^_`{|}~▲‹›»—•–‘«)(…][}{=\"\n",
        "table = str.maketrans(dict.fromkeys(str_punc_to_be_removed))\n",
        "\n",
        "def clean_text(msg):\n",
        "#     start_time = time.time()\n",
        "\n",
        "    #print(\"------------ Cleaning Text: clean_text() ---------------\")\n",
        "    msg = re.sub('[^\"|\".join(str_punc_to_be_removed)0-9a-zA-Z“”‘’.,]+' , \" \", msg )  #Anything not in this will be removed\n",
        "    msg = re.sub(\" +\" , \" \" , msg ) #Removing multiple whitespaces\n",
        "    msg = msg.strip() #Removing leading and trailing spaces\n",
        "    msg = msg.replace(\"“\" , '\"' ) #Replacing with appropriate quotes\n",
        "    msg = msg.replace(\"”\" , '\"' ) #Replacing with appropriate quotes\n",
        "    msg = msg.replace(\"‘\" , \"'\" ) #Replacing with appropriate quotes\n",
        "    msg = msg.replace(\"’\" , \"'\" ) #Replacing with appropriate quotes\n",
        "    msg = msg.replace('\\n' , ' ') #Replacing newline with space\n",
        "    #print(\"Cleaned msg:\" , msg)\n",
        "    \n",
        "    processed = []\n",
        "    for w in msg.split():\n",
        "        w = str(w).translate(table)\n",
        "        if w != \"\":\n",
        "            processed.append(w)\n",
        "    \n",
        "#     end_time = time.time()\n",
        "#     print_msg(\"clean_text ended, Time taken:\" + str(end_time-start_time) + \"sec\")\n",
        "    return ' '.join(w for w in processed)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfhTZBpaZUpv",
        "colab_type": "code",
        "outputId": "cfd2bfbe-958c-409c-b941-513596f381c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "url = \"https://timesofindia.indiatimes.com/business/india-business/govt-eyes-17bn-investment-proposals-to-boost-local-manufacturing/articleshow/76166375.cms\"\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "article_text = article.text\n",
        "article_text"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(Representative image)\\n\\nNEW DELHI: The government is considering proposals for investment of $16-17 billion (around Rs 1.25 lakh crore) to boost domestic production of air conditioners and its components, furniture and leather footwear, while looking at options, including duty hikes, to reduce import dependence and push exports.“To increase manufacturing, ‘Make in India’ and employment, priority sectors have been identified and work has started in three — furniture, air conditioner, leather & footwear. Just in case of air conditioners, we import over 30% of our demand. We need to reduce this quickly. Similarly, we have a small share in global exports , despite being the second-largest leather producer,” PM Narendra Modi said at CII’s annual session.Commerce and industry minister Piyush Goyal has held several rounds of talks with a group of CEOs, led by M&M MD Pawan Goenka , with development clusters being the key. “The overall thrust is to give a push to domestic manufacturing once Covid-19 ends,” he told TOI.Others who have been part of the deliberations suggested development of clusters held the key, while ensuring that it doesn’t turn into real estate development. “The opportunity lies in large-scale manufacturing, which you can do, given the resources and labour. The key is to set up clusters and identify agencies which can set them up,” said Mohit Singla, who leads industry body TPCI.For furniture, three-four clusters involving investment of $10-11 billion (over Rs 75,000 crore) have been discussed, while investment of around $6 billion (around Rs 45,000 crore) has been proposed to reduce the dependence on imports, which is as high as 90% in case of compressors and 80-100% for other components.Further investment of over $1 billion has been proposed to scale up the leather footwear business to attract global investors and improve the quality and branding exercise so that Indian exports, which have a meagre 3.5% share, can compete with rivals from China, Vietnam and Indonesia, sources told TOI.For ACs, the government has been advised to increase customs duty on components to discourage imports from China. Steps are also being contemplated to reduce imports from Thailand. Similarly, some duty hikes for wood that goes into making furniture are also being contemplated, with the long-term solution lying in a forestry policy that supports ecology and the economy, something that Vietnam has done successfully.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxHWEfsxdzwb",
        "colab_type": "text"
      },
      "source": [
        "**Cleaning Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOxpKEsvdyrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getSentences(article_text):\n",
        "  text = cleaner(article_text)\n",
        "  text = clean_text(text)\n",
        "  sentences = sent_tokenize(text)\n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVNC3jzgdvv9",
        "colab_type": "code",
        "outputId": "304f27ef-f91a-46c5-e50d-ea1c01423cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "causal_connective_markers = {\n",
        "    \"if \":\"5\",\n",
        "    \"as a result\":\"15\",\n",
        "    \"consequently\":\"5\",\n",
        "    \"hence\":\"10\",\n",
        "    \"if only\":\"10\",\n",
        "    \"in the end\":\"5\",\n",
        "    \"in turn\":\"5\",\n",
        "    \"largely as a result\":\"15\",\n",
        "    \"so that\":\"5\",\n",
        "    \"thereby\":\"10\",\n",
        "    \"thereupon\":\"10\",\n",
        "    \"therefore\":\"10\",\n",
        "    \"thus\":\"10\",\n",
        "    \"as a consequence\":\"15\",\n",
        "    \"in consequence\":\"10\",\n",
        "    \"apparently because\":\"10\",\n",
        "    \"at least partly because\":\"10\",\n",
        "    \"because\":\"5\",\n",
        "    \"in large part because\":\"10\",\n",
        "    \"mainly because\":\"15\",\n",
        "    \"merely because\":\"15\",\n",
        "    \"only because\":\"15\",\n",
        "    \"particularly because\":\"15\",\n",
        "    \"particularly since\":\"10\",\n",
        "    \"primarily because\":\"15\",\n",
        "    \"only when\":\"10\",\n",
        "    \"especially if\":\"15\",\n",
        "    \"only if\":\"10\",\n",
        "    \"particularly if\":\"15\",\n",
        "    \"typically if\":\"15\",\n",
        "} \n",
        "causal_connective_markers_re = re.compile('(%s)' % '|'.join(causal_connective_markers.keys()))\n",
        "causal_connective_markers_re"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "re.compile(r'(if |as a result|consequently|hence|if only|in the end|in turn|largely as a result|so that|thereby|thereupon|therefore|thus|as a consequence|in consequence|apparently because|at least partly because|because|in large part because|mainly because|merely because|only because|particularly because|particularly since|primarily because|only when|especially if|only if|particularly if|typically if)',\n",
              "re.UNICODE)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXmLG0uFip6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getCandidateSentences(sentences):\n",
        "  sent_candidates = []\n",
        "  for sent in sentences:\n",
        "    sent_l = sent.lower()\n",
        "    if len(causal_connective_markers_re.findall(sent_l)) > 0:\n",
        "      sent_candidates.append(sent)\n",
        "\n",
        "  return sent_candidates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPjDeTn_1gJW",
        "colab_type": "code",
        "outputId": "f630e5d8-1a70-4eb3-e0d8-a4d85856c464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "_getCandidateSentences(_getSentences(article_text))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Further investment of over 1 billion has been proposed to scale up the leather footwear business to attract global investors and improve the quality and branding exercise so that Indian exports, which have a meagre 3.5 share, can compete with rivals from China, Vietnam and Indonesia, sources told TOI.For ACs, the government has been advised to increase customs duty on components to discourage imports from China.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Ws3i7YUNcw",
        "colab_type": "code",
        "outputId": "32ab9082-6888-42bd-884d-40153ced52b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(train_folder)\n",
        "articles = read_articles_from_file_list(train_folder)\n",
        "ref_articles_id, ref_span_starts, ref_span_ends, train_gold_labels = read_predictions_from_file(train_labels_file)\n",
        "print(\"Loaded %d annotations from %d articles\" % (len(ref_span_starts), len(set(ref_articles_id))))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets/train-articles\n",
            "Loaded 6129 annotations from 357 articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKajwMRuUNir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getTrainDataFrame(full_text=True):\n",
        "    negative_samples = []\n",
        "    print(len(ref_articles_id))\n",
        "    print(len(ref_span_starts))\n",
        "    print(len(ref_span_ends))\n",
        "    print(len(train_gold_labels))\n",
        "    traindata = []\n",
        "    for idx, strt, end, label in zip(ref_articles_id, ref_span_starts, ref_span_ends, train_gold_labels):\n",
        "        tempdata = {}\n",
        "        tempdata['id'] = idx\n",
        "        tempdata['strt'] = int(strt)\n",
        "        tempdata['end'] = int(end)\n",
        "        tempdata['label'] = label\n",
        "        articletext = articles[idx]\n",
        "        tempdata['text'] = articletext[tempdata['strt']:tempdata['end']]\n",
        "\n",
        "        if full_text == True:\n",
        "            tempdata['full_text'] = tempdata['text'].replace(\"\\n\", \" \")\n",
        "            articletext = articletext.replace(\"\\n\\n\", \"\\n\")\n",
        "            #articletext = cleanText(articletext)\n",
        "            sentences = articletext.split(\"\\n\")\n",
        "            for s in sentences:\n",
        "                if s.find(tempdata[\"text\"]) != -1:\n",
        "                    tempdata['full_text'] = s\n",
        "                elif len(s) > 0 and random.randint(1,10) > 6 and len(negative_samples) < 4000:\n",
        "                    negative_samples.append(s)\n",
        "        traindata.append(tempdata)\n",
        "    alltrainId = []\n",
        "    alltrainfulltext = []\n",
        "    alltraintext = []\n",
        "    alltrainlabel = []\n",
        "\n",
        "    for d in traindata:\n",
        "        alltrainId.append(d['id'])\n",
        "        alltraintext.append(d['text'])\n",
        "        if full_text == True:\n",
        "            alltrainfulltext.append(d['full_text'])\n",
        "        alltrainlabel.append(d['label'])\n",
        "\n",
        "    alltrain = pd.DataFrame()\n",
        "    alltrain['id'] = alltrainId\n",
        "    if full_text == True:\n",
        "        alltrain['full_text'] = alltrainfulltext\n",
        "    alltrain['Text'] = alltraintext\n",
        "    alltrain['Label'] = alltrainlabel\n",
        "    return alltrain, negative_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgvnO_ITUNlq",
        "colab_type": "code",
        "outputId": "66b73fb3-60c1-45e5-f53a-7e5326bee152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "train_data, negative_samples = _getTrainDataFrame()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6129\n",
            "6129\n",
            "6129\n",
            "6129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Du13JHIUNpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Causal_Oversimplification_df = train_data[train_data[\"Label\"] == \"Causal_Oversimplification\"]\n",
        "Causal_Oversimplification_df = Causal_Oversimplification_df[[\"full_text\", \"Label\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJJJeWbyUNtz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "fb96c15d-e31a-4019-a4fb-984a1fac9b3a"
      },
      "source": [
        "temp = pd.DataFrame()\n",
        "temp[\"full_text\"] = negative_samples\n",
        "temp[\"Label\"] = [0 for i in range(0, len(negative_samples))]\n",
        "print(\"Positive Class Length :\", len(Causal_Oversimplification_df))\n",
        "print(\"Negative Class Length :\", len(temp))\n",
        "dataset = pd.concat([Causal_Oversimplification_df, temp])\n",
        "dataset = shuffle(dataset)\n",
        "dataset = dataset.replace(\"Causal_Oversimplification\", 1)\n",
        "print(\"Combined Length :\", len(dataset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Class Length : 209\n",
            "Negative Class Length : 4000\n",
            "Combined Length : 4209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC--j_A6UNyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = dataset[:3000]\n",
        "test = dataset[3000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFm1mfq4UN2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "a98beae0-6b4a-4277-d48a-967e66868ecc"
      },
      "source": [
        "train.hist()\n",
        "test.hist()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f494460ea20>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwBtHDuEUN5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5D3JutAUN8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77a87a38-c0d9-458f-ac26-a4ffebc9cc07"
      },
      "source": [
        "import tensorflow as tf\n",
        "#!pip uninstall tensorflow==2.2.0\n",
        "!pip install tensorflow==1.15.0\n",
        "!pip install bert-tensorflow"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.2.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.29.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.34.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (47.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=110472b6b2f6619454996e8469c8c75a18cd04a2f6cbe1a223d9423af0d8ddf5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agvPCxB4j593",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "16ced189-f96c-4c56-efdc-a1c4d02055fe"
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNaQGpPMkWFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = \"full_text\"\n",
        "LABEL_COLUMN = \"Label\"\n",
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elVUIhyUkWOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "2198c11b-8b55-4aa8-8481-93e8814dcf24"
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHqKqgkKkWRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "label_list = [0, 1]\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM_XytEikWUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF7_f3TekWXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW0KjiqykWej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100\n",
        "OUTPUT_DIR = \"checkpoints_Casual/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxbA_fbLkWk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6pLYEIgkWoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea24RvAZlkIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "c795df71-3b70-44c7-f392-b69cb5fdc64f"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'checkpoints_Casual/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f48fbaa80f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'checkpoints_Casual/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f48fbaa80f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3CWWVMElnsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgOaCoAclnvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c601f78-8c3e-4ab7-cca5-eae60465b77c"
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-24-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-24-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into checkpoints_Casual/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into checkpoints_Casual/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.640681, step = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.640681, step = 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.592528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.592528\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.05887012, step = 100 (168.771 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.05887012, step = 100 (168.771 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.674733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.674733\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.099147536, step = 200 (148.205 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.099147536, step = 200 (148.205 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 281 into checkpoints_Casual/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 281 into checkpoints_Casual/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.003159853.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.003159853.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:08:16.208207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kazN25vlnya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kFMO2fCln1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "fc622de8-704b-47af-9b7f-cd3cda94ea8c"
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2020-06-03T09:59:16Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2020-06-03T09:59:16Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints_Casual/model.ckpt-281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints_Casual/model.ckpt-281\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2020-06-03-09:59:44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2020-06-03-09:59:44\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 281: auc = 0.7876003, eval_accuracy = 0.9818031, f1_score = 0.7027027, false_negatives = 19.0, false_positives = 3.0, global_step = 281, loss = 0.07484029, precision = 0.8965517, recall = 0.5777778, true_negatives = 1161.0, true_positives = 26.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 281: auc = 0.7876003, eval_accuracy = 0.9818031, f1_score = 0.7027027, false_negatives = 19.0, false_positives = 3.0, global_step = 281, loss = 0.07484029, precision = 0.8965517, recall = 0.5777778, true_negatives = 1161.0, true_positives = 26.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 281: checkpoints_Casual/model.ckpt-281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 281: checkpoints_Casual/model.ckpt-281\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.7876003,\n",
              " 'eval_accuracy': 0.9818031,\n",
              " 'f1_score': 0.7027027,\n",
              " 'false_negatives': 19.0,\n",
              " 'false_positives': 3.0,\n",
              " 'global_step': 281,\n",
              " 'loss': 0.07484029,\n",
              " 'precision': 0.8965517,\n",
              " 'recall': 0.5777778,\n",
              " 'true_negatives': 1161.0,\n",
              " 'true_positives': 26.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKp8qcz9ln4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = [\"Negative\", \"Positive\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, np.exp(prediction['probabilities']), labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02WMtgIbln7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_sentences = [\n",
        "  \"President Trump has been in office for a month and gas prices have been skyrocketing. The rise in gas prices is because of President Trump.\",\n",
        "  \"The reason New Orleans was hit so hard with the hurricane was because of all the immoral people who live there.\",\n",
        "  \"if France had not have declared war on Germany then world war two would have never happened.\",\n",
        "  \"If I apologised for this to China, then China would be the regime that decides what I publish or not.\" ,\n",
        "  \"If the American consumer loses faith, and the coronavirus will be a real test of faith, then a recession is going to happen.\",\n",
        "  \"While respecting the freedom of speech and, in particular, the personal freedom to express one's attitude, CCCD Secretary General John Liu said that the cartoonist “lacked manners and personal qualities such as compassion and sympathy”.\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ6Vf9ppoU1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3318ff92-f96c-41b2-a4a6-28523dee686e"
      },
      "source": [
        "predictions = getPrediction(pred_sentences)\n",
        "predictions"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 6\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] president trump has been in office for a month and gas prices have been sky ##rock ##eti ##ng . the rise in gas prices is because of president trump . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] president trump has been in office for a month and gas prices have been sky ##rock ##eti ##ng . the rise in gas prices is because of president trump . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2343 8398 2038 2042 1999 2436 2005 1037 3204 1998 3806 7597 2031 2042 3712 16901 20624 3070 1012 1996 4125 1999 3806 7597 2003 2138 1997 2343 8398 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2343 8398 2038 2042 1999 2436 2005 1037 3204 1998 3806 7597 2031 2042 3712 16901 20624 3070 1012 1996 4125 1999 3806 7597 2003 2138 1997 2343 8398 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the reason new orleans was hit so hard with the hurricane was because of all the im ##moral people who live there . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the reason new orleans was hit so hard with the hurricane was because of all the im ##moral people who live there . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 3114 2047 5979 2001 2718 2061 2524 2007 1996 7064 2001 2138 1997 2035 1996 10047 22049 2111 2040 2444 2045 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 3114 2047 5979 2001 2718 2061 2524 2007 1996 7064 2001 2138 1997 2035 1996 10047 22049 2111 2040 2444 2045 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] if france had not have declared war on germany then world war two would have never happened . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] if france had not have declared war on germany then world war two would have never happened . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2065 2605 2018 2025 2031 4161 2162 2006 2762 2059 2088 2162 2048 2052 2031 2196 3047 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2065 2605 2018 2025 2031 4161 2162 2006 2762 2059 2088 2162 2048 2052 2031 2196 3047 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] if i ap ##olo ##gis ##ed for this to china , then china would be the regime that decides what i publish or not . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] if i ap ##olo ##gis ##ed for this to china , then china would be the regime that decides what i publish or not . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2065 1045 9706 12898 17701 2098 2005 2023 2000 2859 1010 2059 2859 2052 2022 1996 6939 2008 7288 2054 1045 10172 2030 2025 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2065 1045 9706 12898 17701 2098 2005 2023 2000 2859 1010 2059 2859 2052 2022 1996 6939 2008 7288 2054 1045 10172 2030 2025 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] if the american consumer loses faith , and the corona ##virus will be a real test of faith , then a recession is going to happen . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] if the american consumer loses faith , and the corona ##virus will be a real test of faith , then a recession is going to happen . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2065 1996 2137 7325 12386 4752 1010 1998 1996 21887 23350 2097 2022 1037 2613 3231 1997 4752 1010 2059 1037 19396 2003 2183 2000 4148 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2065 1996 2137 7325 12386 4752 1010 1998 1996 21887 23350 2097 2022 1037 2613 3231 1997 4752 1010 2059 1037 19396 2003 2183 2000 4148 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints_Casual/model.ckpt-281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints_Casual/model.ckpt-281\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('President Trump has been in office for a month and gas prices have been skyrocketing. The rise in gas prices is because of President Trump.',\n",
              "  array([0.01250985, 0.9874901 ], dtype=float32),\n",
              "  'Positive'),\n",
              " ('The reason New Orleans was hit so hard with the hurricane was because of all the immoral people who live there.',\n",
              "  array([0.01370376, 0.9862962 ], dtype=float32),\n",
              "  'Positive'),\n",
              " ('if France had not have declared war on Germany then world war two would have never happened.',\n",
              "  array([0.1346831, 0.8653169], dtype=float32),\n",
              "  'Positive'),\n",
              " ('If I apologised for this to China, then China would be the regime that decides what I publish or not.',\n",
              "  array([0.17569135, 0.8243087 ], dtype=float32),\n",
              "  'Positive'),\n",
              " ('If the American consumer loses faith, and the coronavirus will be a real test of faith, then a recession is going to happen.',\n",
              "  array([0.04903575, 0.9509643 ], dtype=float32),\n",
              "  'Positive'),\n",
              " (\"While respecting the freedom of speech and, in particular, the personal freedom to express one's attitude, CCCD Secretary General John Liu said that the cartoonist “lacked manners and personal qualities such as compassion and sympathy”.\",\n",
              "  array([0.9983272 , 0.00167282], dtype=float32),\n",
              "  'Negative')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9dbhjZ5LLaH",
        "colab_type": "text"
      },
      "source": [
        "**Analysis Report of live News Articles**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5Jht4J2oX3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getArticleText(link):\n",
        "  #Pass link object into Article\n",
        "  article = Article(link)\n",
        "  #Call the download and parse methods to download information\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  return article.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqwiRcScv7ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://www.foxnews.com/media/rush-limbaugh-trump-coronavirus-outbreak-united-states'\n",
        "article_text = _getArticleText(link)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0p7ahUk0iny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(article_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otHXkWJn2bxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = cleaner(article_text)\n",
        "text = clean_text(text)\n",
        "sentences = sent_tokenize(text)\n",
        "sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zjbTJgrv7ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _getTwoClassifierOutput(article_text):\n",
        "  candidates = _getCandidateSentences(_getSentences(article_text))\n",
        "  if len(candidates) == 0:\n",
        "    return None\n",
        "  elif len(candidates) == 1:\n",
        "    candidates.append(\"None\")\n",
        "  predictions = getPrediction(candidates)\n",
        "  return predictions\n",
        "\n",
        "def _getModelPrediction(article_text):\n",
        "  candidates = _getSentences(article_text)\n",
        "  if len(candidates) == 0:\n",
        "    return None\n",
        "  elif len(candidates) == 1:\n",
        "    candidates.append(\"None\")\n",
        "  predictions = getPrediction(candidates)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drFjG2zAv7py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R_predictions = _getTwoClassifierOutput(article_text)\n",
        "M_predictions = _getModelPrediction(article_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLWWBZLWv7sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cee97fc4-b763-472b-9d28-76e6a7c5b693"
      },
      "source": [
        "output = set()\n",
        "if R_predictions != None:\n",
        "  for prediction in R_predictions:\n",
        "    if prediction[1][1] > 0.50:\n",
        "      print(prediction)\n",
        "      output.add(prediction[0])\n",
        "\n",
        "if M_predictions != None:\n",
        "  for prediction in M_predictions:\n",
        "    if prediction[1][1] > 0.80:\n",
        "      print(prediction)\n",
        "      output.add(prediction[0])"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('All he has is the power that being the president of the lone superpower in the world gives him.', array([0.08497876, 0.91502124], dtype=float32), 'Positive')\n",
            "('This is what Washington politicians do.', array([0.02786659, 0.9721334 ], dtype=float32), 'Positive')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdrnE3CDv7wc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "37376be4-7970-486d-cce1-47ae56fb62c3"
      },
      "source": [
        "for item in output:\n",
        "  print(item)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At the same time as the Chinese government and the Chinese people are making every effort to combat this unusual and urgent health threat, Jyllands Posten has published a satirical drawing by Niels Bo Bojesen which is an insult to China and hurts the feelings of the Chinese people the Chinese Embassy relayed.\n",
            "If I apologised for this to China, then China would be the regime that decides what I publish or not.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceEka0IHv7nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}